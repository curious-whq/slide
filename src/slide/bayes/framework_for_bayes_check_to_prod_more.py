import json
import numpy as np
import os
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from collections import defaultdict

# ================= é…ç½® =================
cache_file_path = "/home/whq/Desktop/code_list/perple_test/bayes_stat/log_record_bayes.log.cache_sum_70_no_norm_for_graph.jsonl"
litmus_vec_path = "/home/whq/Desktop/code_list/perple_test/bayes_stat/litmus_vector4_two_tower_gt0.log"
SEED = 2025


def get_interaction_features(p_vec, l_vec):
    """æ„é€ äº¤äº’ç‰¹å¾"""
    p = np.array(p_vec)
    v = np.array(l_vec)
    inter = np.outer(p, v).flatten()
    return np.concatenate([p, v, inter])


def calculate_topk_metrics(y_true, y_pred, groups, k_list=[1, 3, 5]):
    """
    è®¡ç®—åˆ†ç»„ Top-K å‡†ç¡®ç‡
    y_true: çœŸå®åˆ†æ•°æ•°ç»„
    y_pred: é¢„æµ‹åˆ†æ•°æ•°ç»„
    groups: å¯¹åº”çš„æ–‡ä»¶åæ•°ç»„ (ç”¨äºåˆ†ç»„)
    """
    # 1. é‡æ–°ç»„ç»‡æ•°æ®: group_data[litmus] = [(true, pred), ...]
    group_data = defaultdict(list)
    for t, p, g in zip(y_true, y_pred, groups):
        group_data[g].append((t, p))

    hits = {k: 0 for k in k_list}
    # æ–°å¢æŒ‡æ ‡ï¼šTop-1 é—æ†¾å€¼ (Regret) - ä¹Ÿå°±æ˜¯é€‰å‡ºæ¥çš„å’Œæœ€å¥½çš„å·®å¤šå°‘
    top1_score_sum = 0
    best_score_sum = 0

    valid_groups = 0

    for g, records in group_data.items():
        # å¦‚æœæµ‹è¯•é›†é‡Œè¿™ä¸ªç»„æ ·æœ¬å¤ªå°‘ï¼Œæ¯”å¦‚å°‘äº5ä¸ªï¼ŒTop-5å°±æ²¡æ„ä¹‰äº†ï¼Œè·³è¿‡
        if len(records) < 2: continue

        valid_groups += 1

        # 1. æ‰¾å‡ºã€çœŸå®ã€‘æœ€ä¼˜è§£
        # æŒ‰çœŸå®åˆ†æ•°æ’åº
        records.sort(key=lambda x: x[0], reverse=True)
        actual_best_score = records[0][0]

        # 2. æ‰¾å‡ºã€æ¨¡å‹ã€‘æ¨èçš„å‰ K ä¸ª
        # æŒ‰é¢„æµ‹åˆ†æ•°æ’åº
        records.sort(key=lambda x: x[1], reverse=True)

        if actual_best_score == 1:
            continue
        # ç»Ÿè®¡ Top-K å‘½ä¸­ç‡
        for k in k_list:
            # å–æ¨¡å‹é¢„æµ‹çš„å‰ k ä¸ª
            candidates = records[:k]
            # æ£€æŸ¥ï¼šè¿™ k ä¸ªé‡Œï¼Œæœ‰æ²¡æœ‰ä¸€ä¸ªäººçš„çœŸå®åˆ†æ•° == çœŸå®æœ€ä¼˜è§£ï¼Ÿ
            # (æ³¨æ„ï¼šå¯èƒ½æœ‰å¤šä¸ªå¹¶åˆ—ç¬¬ä¸€ï¼Œåªè¦å‘½ä¸­å…¶ä¸­ä¸€ä¸ªå°±ç®—å¯¹)
            # ä¸ºäº†é˜²æ­¢æµ®ç‚¹è¯¯å·®ï¼Œç”¨ >= actual - epsilon
            if any(c[0] >= actual_best_score - 1e-6 for c in candidates):
                hits[k] += 1

        # ç»Ÿè®¡ Top-1 å®é™…å¾—åˆ† vs ç†æƒ³å¾—åˆ†
        model_pick_real_score = records[0][0]  # æ¨¡å‹æ’ç¬¬ä¸€çš„é‚£ä¸ªï¼Œå®ƒçš„çœŸå®åˆ†æ•°
        top1_score_sum += model_pick_real_score
        best_score_sum += actual_best_score

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    metrics = {}
    for k in k_list:
        metrics[f"Top-{k} Acc"] = hits[k] / valid_groups

    # å½’ä¸€åŒ–å¾—åˆ†ç‡ (1.0 ä»£è¡¨å®Œç¾ï¼Œæ¯æ¬¡éƒ½é€‰åˆ°äº†æœ€å¥½çš„)
    metrics["Top-1 Efficiency"] = top1_score_sum / (best_score_sum + 1e-6)

    return metrics, valid_groups


def diagnose_topk():
    print("=== å¼€å§‹ Top-K å‘½ä¸­ç‡å®æˆ˜å¯¹æ¯” (Diagnosis) ===")

    # 1. åŠ è½½ Vector
    print("åŠ è½½å‘é‡...")
    litmus_to_vec = {}
    if os.path.exists(litmus_vec_path):
        with open(litmus_vec_path, "r") as f:
            for line in f:
                if ":" in line:
                    try:
                        n, v = line.strip().split(":", 1)
                        litmus_to_vec[n] = eval(v)
                    except:
                        pass

    if not litmus_to_vec:
        print("é”™è¯¯ï¼šæœªåŠ è½½åˆ°å‘é‡ï¼")
        return

    # 2. åŠ è½½æ•°æ®
    print("åŠ è½½å¹¶æ„å»ºç‰¹å¾çŸ©é˜µ...")
    data_objs = []

    if os.path.exists(cache_file_path):
        with open(cache_file_path, "r") as f:
            for line in f:
                if not line.strip(): continue
                try:
                    obj = json.loads(line)
                    if obj['litmus'] in litmus_to_vec:
                        data_objs.append(obj)
                except:
                    pass

    # æ„å»ºçŸ©é˜µ
    X_std = []
    X_int = []
    y = []
    groups = []  # è®°å½•æ¯ä¸€è¡Œå±äºå“ªä¸ªæ–‡ä»¶

    for obj in data_objs:
        p = obj['param']
        l = litmus_to_vec[obj['litmus']]
        s = obj['score']

        X_std.append(list(p) + list(l))
        X_int.append(get_interaction_features(p, l))
        y.append(s)
        groups.append(obj['litmus'])

    X_std = np.array(X_std)
    X_int = np.array(X_int)
    y = np.array(y)
    groups = np.array(groups)

    # 3. åˆ‡åˆ† (å¿…é¡»å¸¦ä¸Š groups ä¸€èµ·åˆ‡)
    print(f"åˆ‡åˆ†æ•°æ®é›† (æ ·æœ¬æ•°: {len(y)})...")
    indices = np.arange(len(y))
    idx_train, idx_test = train_test_split(indices, test_size=0.2, random_state=SEED)

    y_train = y[idx_train]
    y_test = y[idx_test]
    groups_test = groups[idx_test]  # è¯„ä¼°åªçœ‹æµ‹è¯•é›†

    y_train_log = np.log1p(y_train)

    # 4. è®­ç»ƒä¸é¢„æµ‹

    # === Model A: Standard ===
    print(">>> è®­ç»ƒæ¨¡å‹ A (Standard)...")
    model_std = RandomForestRegressor(n_estimators=100, min_samples_leaf=10, max_features= "sqrt", n_jobs=-1, random_state=SEED)
    model_std.fit(X_std[idx_train], y_train_log)
    pred_test_A = np.expm1(model_std.predict(X_std[idx_test]))

    # === Model B: Interaction ===
    print(">>> è®­ç»ƒæ¨¡å‹ B (Interaction)...")
    model_int = RandomForestRegressor(n_estimators=100, min_samples_leaf=10, max_features= "sqrt", n_jobs=-1, random_state=SEED)
    model_int.fit(X_int[idx_train], y_train_log)
    pred_test_B = np.expm1(model_int.predict(X_int[idx_test]))

    # 5. è®¡ç®— Top-K æŒ‡æ ‡
    print("\næ­£åœ¨è®¡ç®— Top-K æŒ‡æ ‡ (åŸºäºæµ‹è¯•é›†å†…çš„æ’åº)...")
    metrics_A, n_groups = calculate_topk_metrics(y_test, pred_test_A, groups_test)
    metrics_B, _ = calculate_topk_metrics(y_test, pred_test_B, groups_test)

    # 6. æŠ¥è¡¨å±•ç¤º
    print("\n" + "=" * 80)
    print(f"Top-K å‡†ç¡®ç‡å¯¹æ¯”æŠ¥å‘Š (æµ‹è¯•é›†è¦†ç›–æ–‡ä»¶æ•°: {n_groups})")
    print("å®šä¹‰ï¼šTop-K Acc = æ¨¡å‹æ¨èçš„å‰Kä¸ªå‚æ•°ä¸­ï¼Œæ˜¯å¦åŒ…å«è¯¥æ–‡ä»¶åœ¨æµ‹è¯•é›†é‡Œçš„ã€çœŸå®ç¬¬ä¸€åã€‘")
    print("=" * 80)
    print(f"{'Metric':<20} | {'Model A (Standard)':<20} | {'Model B (Interaction)':<20} | {'Diff'}")
    print("-" * 80)

    keys = ["Top-1 Acc", "Top-3 Acc", "Top-5 Acc", "Top-1 Efficiency"]
    for k in keys:
        val_A = metrics_A[k]
        val_B = metrics_B[k]
        diff = val_B - val_A

        # æ ¼å¼åŒ–è¾“å‡º
        mark = ""
        if k == "Top-1 Acc" and diff > 0.01: mark = "âœ… æå‡"
        if k == "Top-1 Acc" and diff < -0.01: mark = "ğŸ”» ä¸‹é™"

        print(f"{k:<20} | {val_A:.2%}             | {val_B:.2%}             | {diff:+.2%} {mark}")

    print("=" * 80)

    # ç»“è®º
    print("\nç»“è®ºåˆ†æï¼š")
    if metrics_B["Top-1 Acc"] > metrics_A["Top-1 Acc"]:
        print("1. ç‰¹å¾äº¤å‰ (Interaction) æé«˜äº† Top-1 å‘½ä¸­ç‡ã€‚è¿™è¯´æ˜ä¹˜æ³•ç‰¹å¾å¸®åŠ©æ¨¡å‹æ›´ç²¾å‡†åœ°é”å®šäº†å³°å€¼ã€‚")
        print("   -> æ¨èä½¿ç”¨ Interaction æ¨¡å‹ã€‚")
    elif metrics_B["Top-1 Acc"] < metrics_A["Top-1 Acc"]:
        print("1. ç‰¹å¾äº¤å‰åè€Œé™ä½äº† Top-1 å‘½ä¸­ç‡ã€‚å¯èƒ½æ˜¯ç‰¹å¾è¿‡å¤šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œæˆ–è€…å¼•å…¥äº†å™ªå£°ã€‚")
        print("   -> æ¨èä½¿ç”¨ Standard æ¨¡å‹ (Param + Vec)ã€‚")
    else:
        print("1. ä¸¤è€…åœ¨ Top-1 ä¸Šè¡¨ç°ä¸€è‡´ã€‚")

    print(f"2. å½“å‰æ¨¡å‹çš„ Top-1 æ•ˆç‡ä¸º {metrics_A['Top-1 Efficiency']:.2%}ã€‚")
    print("   (æ„æ€æ˜¯ï¼šå¦‚æœæ¨¡å‹æ²¡é€‰åˆ°ç¬¬ä¸€åï¼Œå®ƒé€‰çš„é‚£ä¸ªå‚æ•°çš„åˆ†æ•°ï¼Œå¹³å‡ä¹Ÿèƒ½è¾¾åˆ°çœŸå®æœ€é«˜åˆ†çš„ç™¾åˆ†ä¹‹å¤šå°‘)")


if __name__ == "__main__":
    diagnose_topk()